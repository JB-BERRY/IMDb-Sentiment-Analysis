{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeanb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import csv\n",
    "\n",
    "# Tokenize, stem and remove stopwords from the combined data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "# Creating MLP\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 200000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "train = pd.read_csv(\"C:/Users/jeanb/train.tsv\",sep=\"\\t\")\n",
    "test = pd.read_csv(\"C:/Users/jeanb/test.tsv\",sep=\"\\t\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the data into a single unit for processing of features\n",
    "features_train = train['Phrase']\n",
    "labels_train = train['Sentiment']\n",
    "features_test = test['Phrase']\n",
    "combined = features_train.append(features_test).values\n",
    "\n",
    "train_length = len(features_train.values)\n",
    "test_length = len(features_test.values)\n",
    "\n",
    "combined_features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeanb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/10\n",
      "140454/140454 [==============================] - 2278s 16ms/step - loss: 0.3175 - acc: 0.8572 - val_loss: 0.3308 - val_acc: 0.8486\n",
      "Epoch 2/10\n",
      "140454/140454 [==============================] - 2233s 16ms/step - loss: 0.2711 - acc: 0.8763 - val_loss: 0.3285 - val_acc: 0.8507\n",
      "Epoch 3/10\n",
      "140454/140454 [==============================] - 2105s 15ms/step - loss: 0.2518 - acc: 0.8861 - val_loss: 0.3296 - val_acc: 0.8501\n",
      "Epoch 4/10\n",
      "140454/140454 [==============================] - 2032s 14ms/step - loss: 0.2373 - acc: 0.8928 - val_loss: 0.3378 - val_acc: 0.8483\n",
      "Epoch 5/10\n",
      "140454/140454 [==============================] - 2006s 14ms/step - loss: 0.2259 - acc: 0.8981 - val_loss: 0.3410 - val_acc: 0.8480\n",
      "Epoch 6/10\n",
      "140454/140454 [==============================] - 1995s 14ms/step - loss: 0.2164 - acc: 0.9023 - val_loss: 0.3498 - val_acc: 0.8446\n",
      "Epoch 7/10\n",
      "140454/140454 [==============================] - 2028s 14ms/step - loss: 0.2076 - acc: 0.9062 - val_loss: 0.3583 - val_acc: 0.8425\n",
      "Epoch 8/10\n",
      "140454/140454 [==============================] - 2008s 14ms/step - loss: 0.1995 - acc: 0.9103 - val_loss: 0.3713 - val_acc: 0.8392\n",
      "Epoch 9/10\n",
      "140454/140454 [==============================] - 2014s 14ms/step - loss: 0.1923 - acc: 0.9135 - val_loss: 0.3768 - val_acc: 0.8364\n",
      "Epoch 10/10\n",
      "140454/140454 [==============================] - 2006s 14ms/step - loss: 0.1852 - acc: 0.9169 - val_loss: 0.3878 - val_acc: 0.8353\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "punc=list(set(punctuation))\n",
    "stop_words.extend(punc)\n",
    "stop_words.extend([\"'s\", \"'d\", \"'m\"])\n",
    "#print(stop_words)\n",
    "\n",
    "for x in combined:\n",
    "    x=word_tokenize(x)\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    x=[(stemmer.stem(i)).lower() for i in x]\n",
    "    x=[i for i in x if x not in stop_words]\n",
    "    combined_features.append(x)\n",
    "\n",
    "\n",
    "# mapping frequencies with words\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(combined_features)\n",
    "#print(dictionary)\n",
    "\n",
    "id=[]\n",
    "for x in combined_features:\n",
    "    temp = [dictionary.token2id[j] for j in x]\n",
    "    id.append(temp)\n",
    "\n",
    "# using gpu to increase computation speed\n",
    "with tf.device('/gpu:0'):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # padding the input to ensure a fixed size input to the network\n",
    "    x_train=sequence.pad_sequences(np.array(id[:train_length]))\n",
    "    x_test=sequence.pad_sequences(np.array(id[train_length:]))\n",
    "\n",
    "    # one hot encoding\n",
    "    y_train=np_utils.to_categorical(labels_train)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,batch_size=batch_size,epochs=10,validation_split=0.1)\n",
    "\n",
    "    preds = model.predict_classes(x_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_preds(preds, fname):\n",
    "    pd.DataFrame({\"PhraseID\": test['PhraseId'],\"Sentiment\": preds}).to_csv(fname, index=False, header=True)\n",
    "        \n",
    "write_preds(preds, \"C:/Users/jeanb/result-7.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model scores around **0.630** when submitted on the link of the competition (https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenvt",
   "language": "python",
   "name": "newenvt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
